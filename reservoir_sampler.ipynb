{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a random non-stratified sample from all files in the no retweets or quotes folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version with Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: 2016-04-15_without_quotes.json\n",
      "processing: 2016-04-16_without_quotes.json\n",
      "processing: 2016-04-17_without_quotes.json\n",
      "processing: 2016-04-18_without_quotes.json\n",
      "processing: 2016-04-19_without_quotes.json\n",
      "processing: 2016-04-20_without_quotes.json\n",
      "processing: 2016-04-22_without_quotes.json\n",
      "processing: 2016-04-23_without_quotes.json\n",
      "processing: 2016-04-24_without_quotes.json\n",
      "processing: 2016-04-25_without_quotes.json\n",
      "processing: 2016-04-26_without_quotes.json\n",
      "processing: 2016-04-27_without_quotes.json\n",
      "processing: 2016-04-28_without_quotes.json\n",
      "processing: 2016-04-29_without_quotes.json\n",
      "processing: 2016-04-30_without_quotes.json\n",
      "processing: 2016-05-01_without_quotes.json\n",
      "processing: 2016-05-02_without_quotes.json\n",
      "processing: 2016-05-03_without_quotes.json\n",
      "processing: 2016-05-04_without_quotes.json\n",
      "processing: 2016-05-05_without_quotes.json\n",
      "processing: 2016-05-06_without_quotes.json\n",
      "processing: 2016-05-07_without_quotes.json\n",
      "processing: 2016-05-08_without_quotes.json\n",
      "processing: 2016-05-09_without_quotes.json\n",
      "processing: 2016-05-10_without_quotes.json\n",
      "processing: 2016-05-11_without_quotes.json\n",
      "processing: 2016-05-12_without_quotes.json\n",
      "processing: 2016-05-13_without_quotes.json\n",
      "processing: 2016-05-14_without_quotes.json\n",
      "processing: 2016-05-15_without_quotes.json\n",
      "processing: 2016-05-16_without_quotes.json\n",
      "processing: 2016-05-17_without_quotes.json\n",
      "processing: 2016-05-18_without_quotes.json\n",
      "processing: 2016-05-19_without_quotes.json\n",
      "processing: 2016-05-20_without_quotes.json\n",
      "processing: 2016-05-21_without_quotes.json\n",
      "processing: 2016-05-22_without_quotes.json\n",
      "processing: 2016-05-23_without_quotes.json\n",
      "processing: 2016-05-24_without_quotes.json\n",
      "processing: 2016-05-25_without_quotes.json\n",
      "processing: 2016-05-26_without_quotes.json\n",
      "processing: 2016-05-27_without_quotes.json\n",
      "processing: 2016-05-28_without_quotes.json\n",
      "processing: 2016-05-29_without_quotes.json\n",
      "processing: 2016-05-30_without_quotes.json\n",
      "processing: 2016-05-31_without_quotes.json\n",
      "processing: 2016-06-01_without_quotes.json\n",
      "processing: 2016-06-02_without_quotes.json\n",
      "processing: 2016-06-03_without_quotes.json\n",
      "processing: 2016-06-04_without_quotes.json\n",
      "processing: 2016-06-05_without_quotes.json\n",
      "processing: 2016-06-06_without_quotes.json\n",
      "processing: 2016-06-07_without_quotes.json\n",
      "processing: 2016-06-08_without_quotes.json\n",
      "processing: 2016-06-09_without_quotes.json\n",
      "processing: 2016-06-10_without_quotes.json\n",
      "processing: 2016-06-11_without_quotes.json\n",
      "processing: 2016-06-12_without_quotes.json\n",
      "processing: 2016-06-13_without_quotes.json\n",
      "processing: 2016-06-14_without_quotes.json\n",
      "processing: 2016-06-15_without_quotes.json\n",
      "processing: 2016-06-16_without_quotes.json\n",
      "processing: 2016-06-17_without_quotes.json\n",
      "processing: 2016-06-18_without_quotes.json\n",
      "processing: 2016-06-19_without_quotes.json\n",
      "processing: 2016-06-20_without_quotes.json\n",
      "processing: 2016-06-21_without_quotes.json\n",
      "processing: 2016-06-22_without_quotes.json\n",
      "processing: 2016-06-23_without_quotes.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "sample_size = 4000\n",
    "result = [] #The reservoir.\n",
    "N = 0\n",
    "hashtags = set() #A set of desired hashtags.\n",
    "\n",
    "with open(\"partisan_hashtags.txt\", \"r\") as f:\n",
    "    hashtags = set(l[:-1] for l in f.readlines()) #Each line in the .text file ends with /n (new line), so have to cut off the last two characters with [:-1].\n",
    "\n",
    "def hashtag_text(hashtag):\n",
    "    return hashtag[\"text\"].lower()\n",
    "\n",
    "def has_hashtag(tweet):\n",
    "    tweet_hashtags = set(map(hashtag_text, tweet[\"entities\"][\"hashtags\"]))\n",
    "    desired_hashtags = tweet_hashtags & hashtags #Intersection of the two sets.\n",
    "    return len(desired_hashtags) >= 1 #Returns true if there is at least one desired hashtag in the tweet.\n",
    "\n",
    "def is_a_reply(tweet):\n",
    "    return tweet[\"in_reply_to_status_id\"] is not None #Returns tweets that are not replies.\n",
    "\n",
    "for filename in os.listdir(\".\"):\n",
    "    if os.path.isfile(filename) and filename.endswith(\".json\"):\n",
    "        print(\"processing: \" + filename)\n",
    "        with open(filename, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith('{\"limit\":{'):\n",
    "                    continue\n",
    "                tweet = json.loads(line)\n",
    "                if not has_hashtag(tweet) or is_a_reply(tweet): #Filters out tweets that do not contain a hashtag or are replies. \n",
    "                    continue\n",
    "                if N < sample_size:\n",
    "                    N += 1\n",
    "                    result.append(line)\n",
    "                else:\n",
    "                    N += 1\n",
    "                    s = random.randint(0, N-1)\n",
    "                    if s < sample_size:\n",
    "                        result[s] = line\n",
    "                        \n",
    "with open(\"reservoir_sample_4000.json\", \"w\") as outfile:\n",
    "    outfile.write(\"\".join(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version without Functions (Does not work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "sample_size = 4000\n",
    "result = [] #The reservoir.\n",
    "N = 0\n",
    "desired_hashtags = set()\n",
    "\n",
    "for filename in os.listdir(\".\"):\n",
    "    if os.path.isfile(filename) and filename.endswith(\".json\"):\n",
    "        print(\"processing: \" + filename)\n",
    "        with open(filename, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    parsed_line = json.loads(line) #Parse the JSON (otherwise it is just text).\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if parsed_line[\"in_reply_to_status_id\"] or not parsed_line[\"entities\"][\"hashtags\"]:\n",
    "                    continue\n",
    "                if not len(desired_hashtags & set(parsed_line[\"entities\"][\"hashtags\"][\"text\"].lower())) >= 1:\n",
    "                    continue\n",
    "                N += 1\n",
    "                if len(result) < sample_size:\n",
    "                    if line.startswith('{\"limit\":{'):\n",
    "                        N -= 1 #As lines beginning with \"limit\" are skipped, N should not be incremented for them (would bias the results).\n",
    "                        continue #Proceed with the next iteration of the loop (not executing the rest of the loop).\n",
    "                    result.append(line)\n",
    "                else:\n",
    "                    s = int(random.random() * N)\n",
    "                    if s < sample_size: \n",
    "                        if line.startswith('{\"limit\":{'):\n",
    "                            N -= 1\n",
    "                            continue\n",
    "                        result[s] = line\n",
    "                        \n",
    "with open(\"4000_sample.json\", \"w\") as outfile:\n",
    "    outfile.write(\"\".join(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the list of hashtags and convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = [\"#otherref\", \"#HearOtherVoices\", \"#CatsAgainstBrexit\", \"#CatsforBrexit\", \"#Cats4Brexit\", \"#Cats4Britain\", \"#PetsforBritain\", \"#Pets4Britain\", \"#Pets4Brexit\", \"#PetsforBrexit\", \"#PetsAgainstBrexit\", \"#Mutts4Remain\", \"#DogsAgainstBrexit\", \"#DogsforBrexit\", \"#Dogs4Brexit\", \"#Dogs4Britain\", \"#BrexitBusTour\", \"#standupforeurope\", \"#votin\", \"#hugabrit\", \"#whyvoteleave\", \"#BrexitLies\", \"#BrexitAndChill\", \"#BrexitTheMovie\", \"#GoodbyeEU\", \"#LeaveEurope\", \"#BadBrexit\", \"#Brexiteer\", \"#BrexitFantasy\", \"#BritsDontQuit\", \"#CloutNotOut\", \"#LeadingNotLeaving\", \"#BankersLoveBrussels\", \"#fishingforleave\", \"#wewantourfishback\", \"#britinds4in\", \"#EUisTheProblem\", \"#LeftLeave\", \"#LeaveAlliance\", \"#TheLeaveAlliance\", \"#43brokenpromises\",  \"#beleave\", \"#BeLeaver\", \"#BelfastGO\", \"#BetterIn\", \"#betterineu\", \"#betterineurope\", \"#BetterOffIn\", \"#BetterOffOut\", \"#betterout\", \"#bettertogether\", \"#bolstertheborders\", \"#Bremain\", \"#Brexin\", \"#Brexitfears\", \"#Brexitnow\", \"#Brexitrisks\", \"#brexodus\", \"#britainfirst\", \"#BritainIn\", \"#BritainOut\",  \"#britin\", \"#ByeByeEU\", \"#campaigntoremain\", \"#Conservatives4Britain\", \"#ConservativesforBritain\", \"#ConservativesIn\", \"#davesdodgydeal\", \"#DontWalkAway\",      \"#eugood4uk\", \"#EUin\", \"#EUleave\", \"#EUout\", \"#EURefIn\", \"#EURefOut\", \"#EUremain\",  \"#EUstay\", \"#exiteu\", \"#fucktheEU\", \"#GetBritainOut\", \"#goactionday\", \"#GOSuperSaturday\", \"#grassroots_out\", \"#grassrootsout\", \"#grassrootsoutgo\", \"#greenerin\", \"#incampaign\",  \"#Intogether\", \"#justnotintoEU\", \"#labourgo\", \"#LabourIn\", \"#LabourInForBritain\", \"#LabourLeave\", \"#LeadNotLeave\", \"#Leave\", \"#leavechaos\", \"#LeaveEU\", \"#leaveeuofficial\", \"#leavetheeu\", \"#leaveuk\", \"#LeavingEU\", \"#LetsTakeBackControl\", \"#London4Europe\", \"#LoveEuropeLeaveEU\", \"#makebritaingreatagain\", \"#no2brexit\", \"#No2EU\", \"#nobrexit\", \"#notbetteroffout\", \"#nothankseu\", \"#notintoeu\", \"#notobrexit\", \"#NotoEU\", \"#outcampaign\", \"#outeu\", \"#pleasevoteleave\", \"#proeu\", \"#proeurope\", \"#projectbullshit\", \"#projectcheer\", \"#ProjectFact\", \"#projectfantasy\", \"#ProjectFear\", \"#projecthope\",  \"#ProjectNasty\", \"#projectpanic\", \"#projectsmear\", \"#projecttruth\", \"#projectwhinge\", \"#Remain\", \"#remainandgain\", \"#remainer\", \"#remainers\", \"#RemainEU\", \"#remainiac\", \"#remainiacs\", \"#Remainian\", \"#remainians\", \"#remainin\", \"#RemaininEU\", \"#saferout\",  \"#SayNo2EU\", \"#SayNoToEU\", \"#SayYesToEU\", \"#stayEU\", \"#stayin\", \"#StayinEU\", \"#StayinEurope\", \"#StoptheEU\", \"#StrongerIn\", \"#strongerout\", \"#studentsin\", \"#uk4eu\",  \"#UKin\", \"#UKinEU\", \"#UKinEurope\", \"#UKout\", \"#UKoutEU\", \"#UKtoStay\", \"#unisforeu\",  \"#vote_leave\", \"#vote2stay\", \"#VoteIn\", \"#VoteLeave\", \"#VoteLeaveEU\", \"#VoteOut\", \"#VoteRemain\", \"#VoteStay\", \"#VoteToLeave\", \"#womenforbritain\", \"#Yes2EU\", \"#yeseu\", \"#YestoEU\", \"#RisksofStaying\", \"#takecontrol\", \"#WeAreEurope\", \"#infor\", \"#HealthierIn\", \"#Lexit\", \"#ImLeaveBecause\", \"#flexcit\", \"#BritainInEurope\", \"#StudentsGO\", \"#BusinessGO\"]\n",
    "\n",
    "#Sort in alphabetical order and make each item in the list lowercase, starting from the second character (as the first character is a #).\n",
    "#This produces a list; each hashtag is in quotes.\n",
    "#sorted(x[1:].lower() for x in l)\n",
    "\n",
    "#This prints only the items in the list (no commas, no quotes). Easy export to .txt file.\n",
    "for y in sorted(x[1:].lower() for x in l):\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of script that opens the hashtag .txt file as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = []\n",
    "with open(\"partisan_hashtags.txt\", \"r\") as f:\n",
    "    h = [l[:-1] for l in f.readlines()]\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
